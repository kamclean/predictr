---
title: "Predictive Modelling"
output: md_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T);
library(dplyr);library(predictr)
```

# Vignette: Predictive Model Derivation and Validation

## Overview

Predictive modelling is a fantastically useful statistical technique to estimate how likely a patient is to have an event, based on the characteristics of patients similar to them. There are three general scenarios where you'd want to do predictive modelling:

  1. Development of a novel prediction model.
  
  2. Validation of a novel prediction model.
  
  3. (External) Validation of a previous prediction model


## Load data

Firstly let's load our example data - we will be using the survival::colon dataset as an example.

```{r}
data <- tibble::as_tibble(survival::colon) %>%
  dplyr::filter(etype==2) %>% # Outcome of interest is death
  dplyr::filter(rx!="Obs") %>%  # rx will be our binary treatment variable
  dplyr::select(-etype,-study, -status) %>% # Remove superfluous variables
  
  # Convert into numeric and factor variables
  dplyr::mutate_at(vars(obstruct, perfor, adhere, node4), function(x){factor(x, levels=c(0,1), labels = c("No", "Yes"))}) %>%
  dplyr::mutate(rx = factor(rx),
                mort365 = cut(time, breaks = c(-Inf, 365, Inf), labels = c("Yes", "No")),
                mort365 = factor(mort365, levels = c("No", "Yes")),
                sex = factor(sex, levels=c(0,1), labels = c("Female", "Male")),
                differ = factor(differ, levels = c(1,2,3), labels = c("Well", "Moderate", "Poor")),
                extent = factor(extent, levels = c(1,2,3, 4), labels = c("Submucosa", "Muscle", "Serosa", "Contiguous Structures")),
                surg = factor(surg, levels = c(0,1), labels = c("Short", "Long")))

head(data, 10) %>% knitr::kable()
```


Now let's split this very simply into development and validation datasets - this should be done far more robustly in practice!

```{r}
data_dev = data %>% head(0.5 * nrow(data))
data_val = data %>% tail(0.5 * nrow(data))
```


## Predictive Modelling


### 1. Development of a novel prediction model

Now let's say we want to create a new logistic regression model (`fit`) to predict our event (death at 1 year aka "mort365") based on patient and operative factors.

- We're skipping over the part of how you select your explanatory variables as that's not the focus of this package given it requires domain-specific clinical insight.


```{r, echo = T, include = T, message=F, warning=F, error=F}
fit <- finalfit::glmmulti(data_dev, dependent = "mort365", explanatory = c("rx", "sex","obstruct", "differ"))

summary(fit)
```


Now we want to get the patient-level predicted risk of the outcome based on the model - this is needed to evaluate the performance of the model you have derived (see the next vignettes). This isn't necessarily difficult to do in R, but you need to know how to do this.

Using the traditional approach using tidyverse code, you might need to do something like this:


```{r, echo = T}
# Traditional approach
data %>%
      dplyr::select(all_of(c("mort365", c("rx", "sex","obstruct", "differ")))) %>%
      tidyr::drop_na() %>%
      dplyr::mutate(predict_raw = predict(fit, newdata  = ., ),
                    predict_prop = predict(fit, type = "response", newdata  = ., )) %>% 
  head(10) %>%
  knitr::kable() %>% kableExtra::scroll_box(width = 400)
```


The exact same can easily be achieved using the `predictr()` function. You simply provide the data (`data_dev`), and the model (`fit`), and this will be done automatically.


```{r}
# PredictR approach
predictr(data = data_dev, fit = fit) %>% 
  head(10) %>%
  knitr::kable() %>% kableExtra::scroll_box(width = 400)
```


### 2. Validation of a novel prediction model 

While people often stop at deriving a new model, you should be testing whether the model derived (`fit`) is valid on new data. 

With `predictr()` this can again be done simply by supplying the new data (`data_val`) to the function (keeping `fit` unchanged).

- Please note that you **must** have the original 'fit' object for this approach to work.


```{r}
predictr(data = data_val, fit = fit) %>% 
  head(10) %>%
  knitr::kable() %>% kableExtra::scroll_box(width = 400)
```

Once again, this output can be used for model evaluation using the functions described in subsequent vignettes.


## 3. (External) Validation of a previous prediction model

Now let's say someone else has developed a model that you now want to validate (or vice versa). It's unlikely the R fit object will be shared in this instance (they may not have used R, and even if they did the fit object contains a lot of potentially sensitive patient data). 

You will often need to use the model coefficents and intercept provided in a paper to be able to reproduce.


#### Deriving coefficents from fit objects

The normal fit object has coefficients stored in it, but not necessarily in a massively useful / informative format:


```{r}
fit$coefficients
```

Instead you can use the `predictr::coefficient()` function to get this information out in a useful and shareable format. 


```{r}
coefficient(fit) %>% knitr::kable()
```


This provides all the information required for subsequent external validation using the `predictr()` function, and can be used as an alternative to the `fit` parameter.


#### Prediction using model coefficients

For coefficents to be used within `predictr()`, this must be in the format provided by the `coefficient()` function (whether using `coefficient(fit)` or manually extracting from a publication to create the required table). 


```{r}
predictr(data = data_val,
         coefficient = coefficient(fit)) %>%
  head(10) %>% knitr::kable()
```

The default approach in `predictr()` is to use beta-coefficents, however if the paper you have only supplies odds ratios (OR), you can specify this and `predictr()` will handle this internally to produce the appropriate predictions.


```{r}
predictr(data = data_val,
         coefficient = coefficient(fit, coefficient = "or")) %>%
  head(10) %>% knitr::kable()
```
